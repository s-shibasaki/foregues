{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### まずは有効なすべてのTickerとTimestampの組合せを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 200\n",
    "RADIUS = 1\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_data(df):\n",
    "    target_indices = []\n",
    "    timestamps = []\n",
    "    \n",
    "    for i in range(WINDOW, len(df)):\n",
    "        target_indices.append(i)\n",
    "        timestamps.append(df.iloc[i].name)\n",
    "        \n",
    "    return target_indices, timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "input_dir = 'history'\n",
    "\n",
    "def process_file(filename, input_dir):\n",
    "    \"\"\"各ファイルを処理する関数\"\"\"\n",
    "    if not filename.endswith('.joblib'):\n",
    "        return None\n",
    "    \n",
    "    file_path = os.path.join(input_dir, filename)\n",
    "    history_data = joblib.load(file_path)\n",
    "    \n",
    "    ticker = history_data['ticker']\n",
    "    history_df = history_data['history_df']\n",
    "    \n",
    "    # データの作成\n",
    "    target_indices, timestamps = generate_data(history_df)\n",
    "    \n",
    "    return {\n",
    "        'tickers': [ticker] * len(target_indices),\n",
    "        'target_indices': target_indices,\n",
    "        'timestamps': timestamps\n",
    "    }\n",
    "\n",
    "# 並列処理の実行\n",
    "files = [f for f in os.listdir(input_dir) if f.endswith('.joblib')]\n",
    "\n",
    "# n_jobs=-1で全CPUコアを使用\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_file)(filename, input_dir) \n",
    "    for filename in tqdm(files, desc=\"Processing files\")\n",
    ")\n",
    "\n",
    "# 結果の結合\n",
    "tickers = []\n",
    "target_indices = []\n",
    "timestamps = []\n",
    "\n",
    "for result in results:\n",
    "    if result is not None:\n",
    "        tickers.extend(result['tickers'])\n",
    "        target_indices.extend(result['target_indices'])\n",
    "        timestamps.extend(result['timestamps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### test_start_timestamp よりも前のデータを学習に使用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "test_start_timestamp = pd.Timestamp(datetime.now() - timedelta(days=365)).tz_localize('Asia/Tokyo')\n",
    "\n",
    "is_train = [timestamp < test_start_timestamp for timestamp in timestamps]\n",
    "train_indices = np.arange(len(tickers))[is_train]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_indices, val_indices = train_test_split(train_indices)\n",
    "\n",
    "is_test = [timestamp >= test_start_timestamp for timestamp in timestamps]\n",
    "test_indices = np.arange(len(tickers))[is_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### データセットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_image(df):\n",
    "    size = len(df)\n",
    "    array = np.zeros((size, size, 3), dtype=np.uint8)\n",
    "\n",
    "    epsilon = 1e-10\n",
    "    log_high = np.log(df['High'].values + epsilon)\n",
    "    log_low = np.log(df['Low'].values + epsilon)\n",
    "    log_open = np.log(df['Open'].values + epsilon)\n",
    "    log_close = np.log(df['Close'].values + epsilon)\n",
    "    \n",
    "    # NaNを含む時刻のマスクを作成\n",
    "    nan_mask = (np.isnan(log_high) | np.isnan(log_low) | \n",
    "                np.isnan(log_open) | np.isnan(log_close))\n",
    "    \n",
    "    global_min = np.nanmin(log_low)\n",
    "    global_max = np.nanmax(log_high)\n",
    "    center = (global_min + global_max) / 2\n",
    "\n",
    "    price_min = center - RADIUS\n",
    "    price_max = center + RADIUS\n",
    "\n",
    "    # 価格を画像のy座標にマッピング（0が上端で高値、size-1が下端で安値）\n",
    "    def price_to_y(price):\n",
    "        # price_maxが上端(0)、price_minが下端(size-1)\n",
    "        y = (price_max - price) / (price_max - price_min) * (size - 1)\n",
    "        # NaNの場合は-1を返す（後で無効化するため）\n",
    "        y = np.where(np.isnan(price), -1, y)\n",
    "        return np.clip(y, -1, size - 1).astype(int)\n",
    "    \n",
    "    # 各時点の価格をy座標に変換\n",
    "    y_high = price_to_y(log_high)\n",
    "    y_low = price_to_y(log_low)\n",
    "    y_open = price_to_y(log_open)\n",
    "    y_close = price_to_y(log_close)\n",
    "    \n",
    "    # x座標（時間軸）のインデックス配列\n",
    "    x_indices = np.arange(size)\n",
    "    \n",
    "    # チャンネル1: HighからLowまでの範囲を255で塗りつぶす\n",
    "    for i in range(size):\n",
    "        if not nan_mask[i]:\n",
    "            # y_high[i]からy_low[i]まで塗りつぶす（y_high <= y_low なので注意）\n",
    "            array[y_high[i]:y_low[i]+1, i, 0] = 255\n",
    "    \n",
    "    # チャンネル2: Open < Closeの場合、OpenからCloseまでを255で塗りつぶす（陽線）\n",
    "    bullish = log_open < log_close  # 上昇（陽線）\n",
    "    for i in range(size):\n",
    "        if bullish[i] and not nan_mask[i]:\n",
    "            # CloseがOpenより高い（yは小さい）\n",
    "            y_min = min(y_close[i], y_open[i])  # 上端\n",
    "            y_max = max(y_close[i], y_open[i])  # 下端\n",
    "            array[y_min:y_max+1, i, 1] = 255\n",
    "    \n",
    "    # チャンネル3: Close < Openの場合、CloseからOpenまでを255で塗りつぶす（陰線）\n",
    "    bearish = log_close < log_open  # 下降（陰線）\n",
    "    for i in range(size):\n",
    "        if bearish[i] and not nan_mask[i]:\n",
    "            # OpenがCloseより高い（yは小さい）\n",
    "            y_min = min(y_open[i], y_close[i])  # 上端\n",
    "            y_max = max(y_open[i], y_close[i])  # 下端\n",
    "            array[y_min:y_max+1, i, 2] = 255\n",
    "    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, tickers, target_indices, timestamps, valid_indices, hist_dir='history'):\n",
    "        self.tickers = tickers\n",
    "        self.target_indices = target_indices\n",
    "        self.timestamps = timestamps\n",
    "        self.hist_dir = hist_dir\n",
    "        self.valid_indices = valid_indices\n",
    "        self.window = WINDOW\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        valid_idx = self.valid_indices[idx]\n",
    "        \n",
    "        ticker = self.tickers[valid_idx]\n",
    "        timestamp = self.timestamps[valid_idx].isoformat()\n",
    "        target_indice = self.target_indices[valid_idx]\n",
    "        \n",
    "        history_df = joblib.load(os.path.join(self.hist_dir, f\"{ticker}.joblib\"))['history_df']\n",
    "        img = create_image(history_df.iloc[target_indice - self.window: target_indice])\n",
    "        target_row = history_df.iloc[target_indice]\n",
    "        target_change = np.log(target_row['Close'] + 1e-10) - np.log(target_row['Open'] + 1e-10)\n",
    "        # nanの場合は0に置換\n",
    "        if np.isnan(target_change):\n",
    "            target_change = 0.0\n",
    "        return img, target_change, ticker, timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(tickers, target_indices, timestamps, train_indices)\n",
    "val_dataset = Dataset(tickers, target_indices, timestamps, val_indices)\n",
    "test_dataset = Dataset(tickers, target_indices, timestamps, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling import SimpleCNNRegressor\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "model = SimpleCNNRegressor()\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### 学習ループ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# NaNチェック用のヘルパー関数\n",
    "def check_nan(tensor, name=\"tensor\"):\n",
    "    \"\"\"テンソルにNaNが含まれているかチェック\"\"\"\n",
    "    if torch.isnan(tensor).any():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_inf(tensor, name=\"tensor\"):\n",
    "    \"\"\"テンソルにInfが含まれているかチェック\"\"\"\n",
    "    if torch.isinf(tensor).any():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# 学習ループ\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (images, targets, _, _) in enumerate(tqdm(dataloader, desc='train')):\n",
    "        images = images.to(device).permute(0, 3, 1, 2).float() / 255.0\n",
    "        targets = targets.to(device).float().unsqueeze(1)\n",
    "        \n",
    "        # 入力データのNaNチェック\n",
    "        if check_nan(images, \"images\") or check_inf(images, \"images\"):\n",
    "            warnings.warn(f\"NaN or Inf detected in images at batch {batch_idx}\")\n",
    "            raise ValueError(f\"NaN or Inf detected in input images at batch {batch_idx}\")\n",
    "        \n",
    "        if check_nan(targets, \"targets\") or check_inf(targets, \"targets\"):\n",
    "            warnings.warn(f\"NaN or Inf detected in targets at batch {batch_idx}\")\n",
    "            raise ValueError(f\"NaN or Inf detected in targets at batch {batch_idx}\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # モデル出力のNaNチェック\n",
    "        if check_nan(outputs, \"outputs\") or check_inf(outputs, \"outputs\"):\n",
    "            warnings.warn(f\"NaN or Inf detected in model outputs at batch {batch_idx}\")\n",
    "            raise ValueError(f\"NaN or Inf detected in model outputs at batch {batch_idx}\")\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 損失のNaNチェック\n",
    "        if check_nan(loss, \"loss\") or check_inf(loss, \"loss\"):\n",
    "            warnings.warn(f\"NaN or Inf detected in loss at batch {batch_idx}\")\n",
    "            raise ValueError(f\"NaN or Inf detected in loss at batch {batch_idx}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # 勾配のNaNチェック（オプション）\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if check_nan(param.grad, f\"gradient of {name}\") or check_inf(param.grad, f\"gradient of {name}\"):\n",
    "                    warnings.warn(f\"NaN or Inf detected in gradients of {name} at batch {batch_idx}\")\n",
    "                    raise ValueError(f\"NaN or Inf detected in gradients of {name} at batch {batch_idx}\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 評価\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets, _, _) in enumerate(tqdm(dataloader, desc='eval')):\n",
    "            images = images.to(device).permute(0, 3, 1, 2).float() / 255.0\n",
    "            targets = targets.to(device).float().unsqueeze(1)\n",
    "            \n",
    "            # 入力データのNaNチェック\n",
    "            if check_nan(images, \"images\") or check_inf(images, \"images\"):\n",
    "                warnings.warn(f\"NaN or Inf detected in images at batch {batch_idx}\")\n",
    "                raise ValueError(f\"NaN or Inf detected in input images at batch {batch_idx}\")\n",
    "            \n",
    "            if check_nan(targets, \"targets\") or check_inf(targets, \"targets\"):\n",
    "                warnings.warn(f\"NaN or Inf detected in targets at batch {batch_idx}\")\n",
    "                raise ValueError(f\"NaN or Inf detected in targets at batch {batch_idx}\")\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # モデル出力のNaNチェック\n",
    "            if check_nan(outputs, \"outputs\") or check_inf(outputs, \"outputs\"):\n",
    "                warnings.warn(f\"NaN or Inf detected in model outputs at batch {batch_idx}\")\n",
    "                raise ValueError(f\"NaN or Inf detected in model outputs at batch {batch_idx}\")\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # 損失のNaNチェック\n",
    "            if check_nan(loss, \"loss\") or check_inf(loss, \"loss\"):\n",
    "                warnings.warn(f\"NaN or Inf detected in loss at batch {batch_idx}\")\n",
    "                raise ValueError(f\"NaN or Inf detected in loss at batch {batch_idx}\")\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 早期停止の設定\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: 改善が見られない場合に待つエポック数\n",
    "            min_delta: 改善とみなす最小の変化量\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            \n",
    "        return self.early_stop\n",
    "\n",
    "# 学習実行\n",
    "num_epochs = 100\n",
    "early_stopping = EarlyStopping(patience=10, min_delta=1e-4)\n",
    "best_model_state = None\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # 損失値のNaNチェック\n",
    "        if torch.isnan(torch.tensor(train_loss)) or torch.isinf(torch.tensor(train_loss)):\n",
    "            raise ValueError(f\"NaN or Inf detected in training loss at epoch {epoch+1}\")\n",
    "        \n",
    "        if torch.isnan(torch.tensor(val_loss)) or torch.isinf(torch.tensor(val_loss)):\n",
    "            raise ValueError(f\"NaN or Inf detected in validation loss at epoch {epoch+1}\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.6f}')\n",
    "        print(f'  Val Loss: {val_loss:.6f}')\n",
    "        \n",
    "        # ベストモデルの保存\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f'  → Best model saved! (val_loss: {val_loss:.6f})')\n",
    "        \n",
    "        # 早期停止のチェック\n",
    "        if early_stopping(val_loss):\n",
    "            print(f'\\nEarly stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training interrupted due to NaN/Inf detection:\")\n",
    "    print(f\"  {str(e)}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # デバッグ情報の出力\n",
    "    print(\"Debug information:\")\n",
    "    print(f\"  Current epoch: {epoch+1}\")\n",
    "    print(f\"  Current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    # モデルのパラメータの状態をチェック\n",
    "    print(\"\\nModel parameter statistics:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param is not None:\n",
    "            print(f\"  {name}:\")\n",
    "            print(f\"    Shape: {param.shape}\")\n",
    "            print(f\"    Mean: {param.mean().item():.6f}\")\n",
    "            print(f\"    Std: {param.std().item():.6f}\")\n",
    "            print(f\"    Min: {param.min().item():.6f}\")\n",
    "            print(f\"    Max: {param.max().item():.6f}\")\n",
    "            print(f\"    Has NaN: {torch.isnan(param).any().item()}\")\n",
    "            print(f\"    Has Inf: {torch.isinf(param).any().item()}\")\n",
    "    \n",
    "    raise\n",
    "\n",
    "# 最良のモデルを復元\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f'Best model restored (val_loss: {best_val_loss:.6f})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foregues",
   "language": "python",
   "name": "foregues"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
