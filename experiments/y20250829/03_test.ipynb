{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling import SimpleCNNRegressor\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "def load_model(model_path, device='cpu'):\n",
    "    from modeling import SimpleCNNRegressor\n",
    "    import torch\n",
    "    \n",
    "    # モデルのインスタンスを作成\n",
    "    model = SimpleCNNRegressor()\n",
    "    \n",
    "    # チェックポイントの読み込み\n",
    "    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "    \n",
    "    # モデルの重みを読み込み\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # デバイスに移動\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 評価モードに設定\n",
    "    model.eval()\n",
    "    \n",
    "    # その他の情報も返す\n",
    "    return model, checkpoint\n",
    "\n",
    "# 使用例\n",
    "model, checkpoint = load_model('saved_models/best_model.pth', device=device)\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']} with val_loss: {checkpoint['best_val_loss']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_data(df):\n",
    "    target_indices = []\n",
    "    timestamps = []\n",
    "    \n",
    "    for i in range(checkpoint['window'], len(df)):\n",
    "        target_indices.append(i)\n",
    "        timestamps.append(df.iloc[i].name)\n",
    "        \n",
    "    return target_indices, timestamps\n",
    "\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "input_dir = 'history'\n",
    "\n",
    "def process_file(filename, input_dir):\n",
    "    \"\"\"各ファイルを処理する関数\"\"\"\n",
    "    if not filename.endswith('.joblib'):\n",
    "        return None\n",
    "    \n",
    "    file_path = os.path.join(input_dir, filename)\n",
    "    history_data = joblib.load(file_path)\n",
    "    \n",
    "    ticker = history_data['ticker']\n",
    "    history_df = history_data['history_df']\n",
    "    \n",
    "    # データの作成\n",
    "    target_indices, timestamps = generate_data(history_df)\n",
    "    \n",
    "    return {\n",
    "        'tickers': [ticker] * len(target_indices),\n",
    "        'target_indices': target_indices,\n",
    "        'timestamps': timestamps\n",
    "    }\n",
    "\n",
    "# 並列処理の実行\n",
    "files = [f for f in os.listdir(input_dir) if f.endswith('.joblib')]\n",
    "\n",
    "# n_jobs=-1で全CPUコアを使用\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_file)(filename, input_dir) \n",
    "    for filename in tqdm(files, desc=\"Processing files\")\n",
    ")\n",
    "\n",
    "# 結果の結合\n",
    "tickers = []\n",
    "target_indices = []\n",
    "timestamps = []\n",
    "\n",
    "for result in results:\n",
    "    if result is not None:\n",
    "        tickers.extend(result['tickers'])\n",
    "        target_indices.extend(result['target_indices'])\n",
    "        timestamps.extend(result['timestamps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "is_test = [timestamp >= checkpoint['test_start_timestamp'] for timestamp in timestamps]\n",
    "test_indices = np.arange(len(tickers))[is_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_image(df):\n",
    "    size = len(df)\n",
    "    array = np.zeros((size, size, 3), dtype=np.uint8)\n",
    "\n",
    "    epsilon = 1e-10\n",
    "    log_high = np.log(df['High'].values + epsilon)\n",
    "    log_low = np.log(df['Low'].values + epsilon)\n",
    "    log_open = np.log(df['Open'].values + epsilon)\n",
    "    log_close = np.log(df['Close'].values + epsilon)\n",
    "    \n",
    "    # NaNを含む時刻のマスクを作成\n",
    "    nan_mask = (np.isnan(log_high) | np.isnan(log_low) | \n",
    "                np.isnan(log_open) | np.isnan(log_close))\n",
    "    \n",
    "    global_min = np.nanmin(log_low)\n",
    "    global_max = np.nanmax(log_high)\n",
    "    center = (global_min + global_max) / 2\n",
    "\n",
    "    price_min = center - checkpoint['radius']\n",
    "    price_max = center + checkpoint['radius']\n",
    "\n",
    "    # 価格を画像のy座標にマッピング（0が上端で高値、size-1が下端で安値）\n",
    "    def price_to_y(price):\n",
    "        # price_maxが上端(0)、price_minが下端(size-1)\n",
    "        y = (price_max - price) / (price_max - price_min) * (size - 1)\n",
    "        # NaNの場合は-1を返す（後で無効化するため）\n",
    "        y = np.where(np.isnan(price), -1, y)\n",
    "        return np.clip(y, -1, size - 1).astype(int)\n",
    "    \n",
    "    # 各時点の価格をy座標に変換\n",
    "    y_high = price_to_y(log_high)\n",
    "    y_low = price_to_y(log_low)\n",
    "    y_open = price_to_y(log_open)\n",
    "    y_close = price_to_y(log_close)\n",
    "    \n",
    "    # x座標（時間軸）のインデックス配列\n",
    "    x_indices = np.arange(size)\n",
    "    \n",
    "    # チャンネル1: HighからLowまでの範囲を255で塗りつぶす\n",
    "    for i in range(size):\n",
    "        if not nan_mask[i]:\n",
    "            # y_high[i]からy_low[i]まで塗りつぶす（y_high <= y_low なので注意）\n",
    "            array[y_high[i]:y_low[i]+1, i, 0] = 255\n",
    "    \n",
    "    # チャンネル2: Open < Closeの場合、OpenからCloseまでを255で塗りつぶす（陽線）\n",
    "    bullish = log_open < log_close  # 上昇（陽線）\n",
    "    for i in range(size):\n",
    "        if bullish[i] and not nan_mask[i]:\n",
    "            # CloseがOpenより高い（yは小さい）\n",
    "            y_min = min(y_close[i], y_open[i])  # 上端\n",
    "            y_max = max(y_close[i], y_open[i])  # 下端\n",
    "            array[y_min:y_max+1, i, 1] = 255\n",
    "    \n",
    "    # チャンネル3: Close < Openの場合、CloseからOpenまでを255で塗りつぶす（陰線）\n",
    "    bearish = log_close < log_open  # 下降（陰線）\n",
    "    for i in range(size):\n",
    "        if bearish[i] and not nan_mask[i]:\n",
    "            # OpenがCloseより高い（yは小さい）\n",
    "            y_min = min(y_open[i], y_close[i])  # 上端\n",
    "            y_max = max(y_open[i], y_close[i])  # 下端\n",
    "            array[y_min:y_max+1, i, 2] = 255\n",
    "    \n",
    "    return array\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, tickers, target_indices, timestamps, valid_indices, hist_dir='history'):\n",
    "        self.tickers = tickers\n",
    "        self.target_indices = target_indices\n",
    "        self.timestamps = timestamps\n",
    "        self.hist_dir = hist_dir\n",
    "        self.valid_indices = valid_indices\n",
    "        self.window = checkpoint['window']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        valid_idx = self.valid_indices[idx]\n",
    "        \n",
    "        ticker = self.tickers[valid_idx]\n",
    "        timestamp = self.timestamps[valid_idx].isoformat()\n",
    "        target_indice = self.target_indices[valid_idx]\n",
    "        \n",
    "        history_df = joblib.load(os.path.join(self.hist_dir, f\"{ticker}.joblib\"))['history_df']\n",
    "        img = create_image(history_df.iloc[target_indice - self.window: target_indice])\n",
    "        target_row = history_df.iloc[target_indice]\n",
    "        target_change = (np.log(target_row['Close'] + 1e-10) - np.log(target_row['Open'] + 1e-10)) * checkpoint['target_change_multiplier']\n",
    "        # nanの場合は0に置換\n",
    "        if np.isnan(target_change):\n",
    "            target_change = 0.0\n",
    "        return img, target_change, ticker, timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "test_dataset = Dataset(tickers, target_indices, timestamps, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_tickers = []\n",
    "    all_timestamps = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images, targets, tickers_batch, timestamps_batch = batch\n",
    "            \n",
    "            # データをデバイスに転送\n",
    "            images = (images.float().to(device) / 255.0).permute(0, 3, 1, 2)\n",
    "            targets = targets.float().to(device)\n",
    "            \n",
    "            # 予測\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # 結果を保存\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_tickers.extend(tickers_batch)\n",
    "            all_timestamps.extend(timestamps_batch)\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_targets), all_tickers, all_timestamps\n",
    "\n",
    "# テストの実行\n",
    "predictions, targets, test_tickers, test_timestamps = test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[12]:\n",
    "# 評価メトリクスの計算\n",
    "mse = mean_squared_error(targets, predictions)\n",
    "mae = mean_absolute_error(targets, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(targets, predictions)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Test Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"MSE:  {mse:.6f}\")\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "print(f\"MAE:  {mae:.6f}\")\n",
    "print(f\"R2:   {r2:.6f}\")\n",
    "\n",
    "# 相関係数\n",
    "correlation = np.corrcoef(targets, predictions[:, 0])[0, 1]\n",
    "print(f\"Correlation: {correlation:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[13]:\n",
    "# 結果の可視化\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. 予測値 vs 実際値の散布図\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(targets, predictions, alpha=0.3, s=1)\n",
    "ax.plot([targets.min(), targets.max()], [targets.min(), targets.max()], 'r--', lw=2)\n",
    "ax.set_xlabel('Actual Change')\n",
    "ax.set_ylabel('Predicted Change')\n",
    "ax.set_title(f'Predictions vs Actual (R2={r2:.4f})')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. 残差プロット\n",
    "residuals = predictions - targets\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(predictions, residuals, alpha=0.3, s=1)\n",
    "ax.axhline(y=0, color='r', linestyle='--')\n",
    "ax.set_xlabel('Predicted Change')\n",
    "ax.set_ylabel('Residuals')\n",
    "ax.set_title('Residual Plot')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. 残差のヒストグラム\n",
    "ax = axes[1, 0]\n",
    "ax.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('Residuals')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(f'Distribution of Residuals (std={np.std(residuals):.6f})')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. 予測値と実際値のヒストグラム\n",
    "ax = axes[1, 1]\n",
    "ax.hist(targets, bins=50, alpha=0.5, label='Actual', color='blue', edgecolor='black')\n",
    "ax.hist(predictions, bins=50, alpha=0.5, label='Predicted', color='red', edgecolor='black')\n",
    "ax.set_xlabel('Change')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Actual vs Predicted')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[14]:\n",
    "# 銘柄別のパフォーマンス分析\n",
    "results_df = pd.DataFrame({\n",
    "    'ticker': test_tickers,\n",
    "    'timestamp': test_timestamps,\n",
    "    'actual': targets,\n",
    "    'predicted': predictions,\n",
    "    'residual': residuals,\n",
    "    'abs_error': np.abs(residuals)\n",
    "})\n",
    "\n",
    "# 銘柄ごとの統計\n",
    "ticker_stats = results_df.groupby('ticker').agg({\n",
    "    'abs_error': ['mean', 'std', 'count'],\n",
    "    'residual': ['mean', 'std'],\n",
    "    'actual': 'std',\n",
    "    'predicted': 'std'\n",
    "}).round(6)\n",
    "\n",
    "ticker_stats.columns = ['_'.join(col).strip() for col in ticker_stats.columns]\n",
    "ticker_stats = ticker_stats.sort_values('abs_error_mean')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Top 10 Best Performing Tickers:\")\n",
    "print(\"=\" * 50)\n",
    "print(ticker_stats.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Top 10 Worst Performing Tickers:\")\n",
    "print(\"=\" * 50)\n",
    "print(ticker_stats.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[15]:\n",
    "# 時系列でのパフォーマンス分析\n",
    "results_df['timestamp'] = pd.to_datetime(results_df['timestamp'])\n",
    "results_df = results_df.sort_values('timestamp')\n",
    "\n",
    "# 日次のパフォーマンス\n",
    "daily_performance = results_df.groupby(results_df['timestamp'].dt.date).agg({\n",
    "    'abs_error': 'mean',\n",
    "    'residual': ['mean', 'std'],\n",
    "    'actual': 'count'\n",
    "}).round(6)\n",
    "\n",
    "daily_performance.columns = ['_'.join(col).strip() if col[1] else col[0] for col in daily_performance.columns]\n",
    "\n",
    "# 時系列プロット\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(daily_performance.index, daily_performance['abs_error'], label='MAE', color='blue')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Mean Absolute Error')\n",
    "ax.set_title('Daily Performance')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.bar(daily_performance.index, daily_performance['actual'], label='Sample Count', color='green', alpha=0.5)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Number of Predictions')\n",
    "ax.set_title('Daily Sample Count')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foregues",
   "language": "python",
   "name": "foregues"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
