{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('jq_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "データフレームdfがあります。  \n",
    "dfにはDisclosedDateというカラムがあります。これが今日のちょうど一年前までを学習用（一部を検証用に分割）、それ以降をテスト用として使用します。  \n",
    "dfには'ftn.'で始まるカラムがあり、これらは数値特徴量です。NaNを欠損値として処理します。  \n",
    "dfには'ftc.'で始まるカラムがあり、これらはカテゴリ特徴量です。Noneを欠損値として処理します。  \n",
    "これらの特徴量をxgboostで学習します。ラベルは'label'というカラム（数値）を使用します。前進法で特徴量選択を行います。アーリーストッピングも実装します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. データの分割\n",
    "def split_data_by_date(df):\n",
    "    \"\"\"\n",
    "    今日の1年前を基準に学習用とテスト用にデータを分割\n",
    "    \"\"\"\n",
    "    today = datetime.now()\n",
    "    one_year_ago = today - timedelta(days=365)\n",
    "    \n",
    "    # DisclosedDateをdatetime型に変換\n",
    "    df['DisclosedDate'] = pd.to_datetime(df['DisclosedDate'])\n",
    "    \n",
    "    # labelのNaNを0に置換\n",
    "    df['label'] = df['label'].fillna(0)\n",
    "    print(f\"labelのNaN件数: {df['label'].isna().sum()} 件 (0に置換済み)\")\n",
    "    \n",
    "    # 学習用とテスト用に分割\n",
    "    train_df = df[df['DisclosedDate'] <= one_year_ago].copy()\n",
    "    test_df = df[df['DisclosedDate'] > one_year_ago].copy()\n",
    "    \n",
    "    print(f\"学習用データ: {len(train_df)} 件 ({train_df['DisclosedDate'].min()} ~ {train_df['DisclosedDate'].max()})\")\n",
    "    print(f\"テスト用データ: {len(test_df)} 件 ({test_df['DisclosedDate'].min()} ~ {test_df['DisclosedDate'].max()})\")\n",
    "    \n",
    "    # labelの分布を確認\n",
    "    print(f\"\\n学習用ラベルの統計:\")\n",
    "    print(f\"  平均: {train_df['label'].mean():.4f}\")\n",
    "    print(f\"  標準偏差: {train_df['label'].std():.4f}\")\n",
    "    print(f\"  最小値: {train_df['label'].min():.4f}\")\n",
    "    print(f\"  最大値: {train_df['label'].max():.4f}\")\n",
    "    print(f\"  ゼロの割合: {(train_df['label'] == 0).mean():.2%}\")\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. データを時系列で分割\n",
    "train_df, test_df = split_data_by_date(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 特徴量の前処理\n",
    "def preprocess_features(train_df, test_df):\n",
    "    \"\"\"\n",
    "    数値特徴量とカテゴリ特徴量の前処理\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # 特徴量カラムの識別\n",
    "    numeric_cols = [col for col in train_df.columns if col.startswith('ftn.')]\n",
    "    categorical_cols = [col for col in train_df.columns if col.startswith('ftc.')]\n",
    "    \n",
    "    print(f\"数値特徴量: {len(numeric_cols)} 個\")\n",
    "    print(f\"カテゴリ特徴量: {len(categorical_cols)} 個\")\n",
    "    \n",
    "    # 処理後のデータフレーム\n",
    "    train_processed = train_df.copy()\n",
    "    test_processed = test_df.copy()\n",
    "    \n",
    "    # 数値特徴量の無限大処理（inf, -infをNaNに変換）\n",
    "    # XGBoostはNaNを自動的にmissingとして扱うため、fillnaは不要\n",
    "    for col in numeric_cols:\n",
    "        # 正の無限大と負の無限大をNaNに置換\n",
    "        train_processed[col] = train_processed[col].replace([np.inf, -np.inf], np.nan)\n",
    "        test_processed[col] = test_processed[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # カテゴリ特徴量の処理\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        # Noneを'missing'に置換\n",
    "        train_processed[col] = train_processed[col].fillna('missing')\n",
    "        test_processed[col] = test_processed[col].fillna('missing')\n",
    "        \n",
    "        # Label Encoding\n",
    "        le = LabelEncoder()\n",
    "        # 学習データで fit\n",
    "        train_values = train_processed[col].astype(str)\n",
    "        le.fit(train_values)\n",
    "        \n",
    "        # 変換\n",
    "        train_processed[col] = le.transform(train_values)\n",
    "        \n",
    "        # テストデータの未知のカテゴリを処理\n",
    "        test_values = test_processed[col].astype(str)\n",
    "        test_values_encoded = []\n",
    "        for val in test_values:\n",
    "            if val in le.classes_:\n",
    "                test_values_encoded.append(le.transform([val])[0])\n",
    "            else:\n",
    "                test_values_encoded.append(-1)  # 未知のカテゴリ\n",
    "        test_processed[col] = test_values_encoded\n",
    "        \n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    return train_processed, test_processed, numeric_cols + categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 特徴量の前処理\n",
    "train_processed, test_processed, feature_cols = preprocess_features(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 学習データをさらに訓練用と検証用に分割\n",
    "X_train_full = train_processed[feature_cols]\n",
    "y_train_full = train_processed['label']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n訓練データ: {len(X_train)} 件\")\n",
    "print(f\"検証データ: {len(X_val)} 件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 効率化された特徴量選択（後退法）\n",
    "def train_xgboost_light(X_train, y_train, X_val, y_val, features, num_boost_round=100):\n",
    "    \"\"\"\n",
    "    軽量版のXGBoost学習（特徴量選択用）\n",
    "    \"\"\"\n",
    "    dtrain = xgb.DMatrix(X_train[features], label=y_train)\n",
    "    dval = xgb.DMatrix(X_val[features], label=y_val)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 3,\n",
    "        'gamma': 0.1,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 1.0,\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cuda',\n",
    "        'seed': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    evals_result = {}\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        evals=[(dval, 'val')],\n",
    "        evals_result=evals_result,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    val_score = evals_result['val']['rmse'][-1]\n",
    "    return val_score, model\n",
    "\n",
    "\n",
    "def remove_all_nan_features(X_train, X_val, features):\n",
    "    \"\"\"\n",
    "    すべてNaNの特徴量を除外する\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : 訓練データ\n",
    "    X_val : 検証データ\n",
    "    features : チェックする特徴量のリスト\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    valid_features : NaNでない有効な特徴量のリスト\n",
    "    nan_features : すべてNaNだった特徴量のリスト\n",
    "    \"\"\"\n",
    "    valid_features = []\n",
    "    nan_features = []\n",
    "    \n",
    "    for feature in features:\n",
    "        # 訓練データと検証データの両方でチェック\n",
    "        train_all_nan = X_train[feature].isna().all() if feature in X_train.columns else True\n",
    "        val_all_nan = X_val[feature].isna().all() if feature in X_val.columns else True\n",
    "        \n",
    "        if train_all_nan or val_all_nan:\n",
    "            nan_features.append(feature)\n",
    "        else:\n",
    "            valid_features.append(feature)\n",
    "    \n",
    "    return valid_features, nan_features\n",
    "\n",
    "\n",
    "def feature_selection_backward(X_train, y_train, X_val, y_val, all_features, \n",
    "                               min_features=50, patience=20, max_degradation=0.001,\n",
    "                               batch_removal=True, improvement_ratio=0.5):\n",
    "\n",
    "    \"\"\"\n",
    "    効率化された後退法による特徴量選択\n",
    "    性能劣化が最小の特徴量から順に除外、劣化が小さい複数特徴量を同時除外可能\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, y_train, X_val, y_val : 学習・検証データ\n",
    "    all_features : 全特徴量のリスト\n",
    "    min_features : 最小特徴量数（これ以下にはしない）\n",
    "    patience : 改善が見られない連続回数の上限\n",
    "    max_degradation : 許容される最大性能劣化（正の値）\n",
    "    batch_removal : 複数特徴量同時除外を有効にするか\n",
    "    improvement_ratio : 性能改善時にバッチに追加する特徴量の改善比率閾値\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 効率化された後退法による特徴量選択開始 ===\")\n",
    "    \n",
    "    # すべてNaNの特徴量を除外\n",
    "    valid_features, nan_features = remove_all_nan_features(X_train, X_val, all_features)\n",
    "    \n",
    "    if nan_features:\n",
    "        print(f\"\\n警告: {len(nan_features)}個の特徴量がすべてNaNのため除外されました\")\n",
    "        print(f\"  除外された特徴量の例（最大10個）: {nan_features[:10]}\")\n",
    "        if len(nan_features) > 10:\n",
    "            print(f\"  ... 他 {len(nan_features)-10}個\")\n",
    "    \n",
    "    print(f\"\\n有効な特徴量数: {len(valid_features)} (元: {len(all_features)})\")\n",
    "    print(f\"最小特徴量数: {min_features}\")\n",
    "    print(f\"Patience: {patience}\")\n",
    "    print(f\"許容最大劣化: {max_degradation}\")\n",
    "    print(f\"バッチ除外: {'有効' if batch_removal else '無効'}\")\n",
    "    \n",
    "    # 初期状態：全ての有効な特徴量を使用\n",
    "    selected_features = valid_features.copy()\n",
    "    \n",
    "    # 初期スコアを計算\n",
    "    print(f\"\\n初期特徴量数: {len(selected_features)}\")\n",
    "    print(\"初期スコアを計算中...\")\n",
    "    best_score, _ = train_xgboost_light(X_train, y_train, X_val, y_val, selected_features)\n",
    "    print(f\"初期スコア: {best_score:.6f}\")\n",
    "    \n",
    "    patience_counter = 0\n",
    "    removed_count = 0\n",
    "    \n",
    "    # 後退法による特徴量選択\n",
    "    print(\"\\n--- 効率化された後退法による特徴量選択 ---\")\n",
    "    \n",
    "    while len(selected_features) > min_features and patience_counter < patience:\n",
    "        candidate_scores = []\n",
    "        \n",
    "        print(f\"\\n特徴量除外候補を評価中... (現在の特徴量数: {len(selected_features)})\")\n",
    "        \n",
    "        # 各特徴量を除外した場合の影響を評価\n",
    "        for feature in tqdm(selected_features, desc=\"候補評価\"):\n",
    "            current_features = [f for f in selected_features if f != feature]\n",
    "            \n",
    "            # 安全チェック: 除外後も有効な特徴量が残るか\n",
    "            if len(current_features) < min_features:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                score, _ = train_xgboost_light(X_train, y_train, X_val, y_val, current_features)\n",
    "                degradation = score - best_score  # 正の値が除外による性能劣化\n",
    "                candidate_scores.append((feature, score, degradation))\n",
    "            except Exception as e:\n",
    "                print(f\"特徴量 {feature} の除外評価でエラー: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not candidate_scores:\n",
    "            print(\"評価可能な候補がありません\")\n",
    "            break\n",
    "        \n",
    "        # 除外による性能劣化が小さい順にソート（劣化が小さい = 除外しても影響が少ない）\n",
    "        candidate_scores.sort(key=lambda x: x[2])\n",
    "        \n",
    "        # 最小劣化\n",
    "        min_degradation = candidate_scores[0][2]\n",
    "        \n",
    "        # 性能が改善または許容範囲内の劣化の場合\n",
    "        if min_degradation <= max_degradation:\n",
    "            features_to_remove = []\n",
    "            \n",
    "            # 複数特徴量同時除外の実行\n",
    "            if batch_removal and min_degradation < 0:  # 性能改善時のみバッチ除外を検討\n",
    "                # 十分に性能を改善した特徴量を特定\n",
    "                improvement_threshold = abs(min_degradation) * improvement_ratio\n",
    "                \n",
    "                improvement_features = [\n",
    "                    (feature, score, degradation) \n",
    "                    for feature, score, degradation in candidate_scores \n",
    "                    if degradation < 0 and abs(degradation) >= improvement_threshold\n",
    "                ]\n",
    "                \n",
    "                print(f\"\\n性能改善候補: {len(improvement_features)}個\")\n",
    "                print(f\"  改善閾値: {improvement_threshold:.6f}\")\n",
    "                for i, (feature, score, degradation) in enumerate(improvement_features[:5]):\n",
    "                    print(f\"    {i+1}. {feature}: 改善 {abs(degradation):.6f} (スコア: {score:.6f})\")\n",
    "                if len(improvement_features) > 5:\n",
    "                    print(f\"    ... 他 {len(improvement_features)-5}個\")\n",
    "                \n",
    "                # 改善特徴量での同時除外を検証\n",
    "                if improvement_features:\n",
    "                    batch_remove_features = [f[0] for f in improvement_features]\n",
    "                    test_features = [f for f in selected_features if f not in batch_remove_features]\n",
    "                    \n",
    "                    if len(test_features) >= min_features:\n",
    "                        try:\n",
    "                            batch_score, _ = train_xgboost_light(X_train, y_train, X_val, y_val, test_features)\n",
    "                            batch_degradation = batch_score - best_score\n",
    "                            \n",
    "                            if batch_degradation < 0:  # さらに性能改善を確認\n",
    "                                selected_features = test_features\n",
    "                                best_score = batch_score\n",
    "                                patience_counter = 0\n",
    "                                removed_count += len(batch_remove_features)\n",
    "                                \n",
    "                                print(f\"バッチ除外成功: {len(batch_remove_features)}個の特徴量を同時除外\")\n",
    "                                print(f\"  除外特徴量: {batch_remove_features[:5]}\")\n",
    "                                if len(batch_remove_features) > 5:\n",
    "                                    print(f\"  ... 他 {len(batch_remove_features)-5}個\")\n",
    "                                print(f\"  現在の特徴量数: {len(selected_features)}\")\n",
    "                                print(f\"  スコア: {best_score:.6f} (改善幅: {abs(batch_degradation):.6f})\")\n",
    "                                continue\n",
    "                        except Exception as e:\n",
    "                            print(f\"バッチ除外でエラー: {e}\")\n",
    "            \n",
    "            # 単一特徴量除外\n",
    "            worst_feature, worst_score, worst_degradation = candidate_scores[0]\n",
    "            \n",
    "            if worst_degradation <= max_degradation:\n",
    "                selected_features.remove(worst_feature)\n",
    "                best_score = worst_score\n",
    "                patience_counter = 0\n",
    "                removed_count += 1\n",
    "                \n",
    "                print(f\"単一特徴量除外: {worst_feature}\")\n",
    "                print(f\"  現在の特徴量数: {len(selected_features)}\")\n",
    "                print(f\"  スコア: {best_score:.6f} (劣化: {worst_degradation:.6f})\")\n",
    "                \n",
    "                # 性能が改善した場合\n",
    "                if worst_degradation < 0:\n",
    "                    print(f\"  ※性能が改善しました！ (改善幅: {abs(worst_degradation):.6f})\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"全ての候補で劣化が許容範囲外 (最小劣化: {worst_degradation:.6f} > {max_degradation:.6f})\")\n",
    "                print(f\"patience: {patience_counter}/{patience}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"全ての候補で劣化が許容範囲外 (最小劣化: {min_degradation:.6f} > {max_degradation:.6f})\")\n",
    "            print(f\"patience: {patience_counter}/{patience}\")\n",
    "    \n",
    "    print(f\"\\n=== 後退法による特徴量選択完了 ===\")\n",
    "    print(f\"最終選択特徴量数: {len(selected_features)} (除外数: {removed_count})\")\n",
    "    print(f\"最終スコア: {best_score:.6f}\")\n",
    "    if nan_features:\n",
    "        print(f\"事前除外されたNaN特徴量数: {len(nan_features)}\")\n",
    "    \n",
    "    # 除外された特徴量のリストも返す\n",
    "    removed_features = [f for f in valid_features if f not in selected_features]\n",
    "    \n",
    "    return selected_features, best_score, removed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 特徴量選択の実行\n",
    "print(f\"元の特徴量数: {len(feature_cols)}\")\n",
    "selected_features, selection_score = feature_selection_backward(\n",
    "    X_train, y_train, X_val, y_val, feature_cols\n",
    ")\n",
    "\n",
    "print(f\"\\n選択された特徴量:\")\n",
    "for i, feature in enumerate(selected_features, 1):\n",
    "    print(f\"  {i:2d}. {feature}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. XGBoostモデルの学習（アーリーストッピング付き）\n",
    "def train_xgboost_with_early_stopping(X_train, y_train, X_val, y_val, selected_features):\n",
    "    \"\"\"\n",
    "    選択された特徴量でXGBoostモデルを学習\n",
    "    \"\"\"\n",
    "    print(\"\\n=== XGBoostモデルの学習（アーリーストッピング付き） ===\")\n",
    "    \n",
    "    # DMatrix形式に変換\n",
    "    dtrain = xgb.DMatrix(X_train[selected_features], label=y_train)\n",
    "    dval = xgb.DMatrix(X_val[selected_features], label=y_val)\n",
    "    \n",
    "    # パラメータ設定\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.05,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 3,\n",
    "        'gamma': 0.1,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 1.0,\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cuda',\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # アーリーストッピング付きで学習\n",
    "    evals_result = {}\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=50,\n",
    "        evals_result=evals_result,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n最適なラウンド数: {model.best_iteration}\")\n",
    "    print(f\"最良の検証スコア (RMSE): {model.best_score:.4f}\")\n",
    "    \n",
    "    return model, evals_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 選択された特徴量でモデルを再学習（アーリーストッピング付き）\n",
    "final_model, evals_result = train_xgboost_with_early_stopping(\n",
    "    X_train, y_train, \n",
    "    X_val, y_val, \n",
    "    selected_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# モデルの保存\n",
    "joblib.dump(final_model, 'jq_model.joblib')\n",
    "joblib.dump(selected_features, 'jq_selected_features.joblib')\n",
    "\n",
    "# 特徴量選択結果の保存\n",
    "feature_selection_results = {\n",
    "    'selected_features': selected_features,\n",
    "    'final_score': selection_score,\n",
    "    'feature_count': len(selected_features),\n",
    "    'original_feature_count': len(feature_cols)\n",
    "}\n",
    "\n",
    "with open('jq_feature_selection_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(feature_selection_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nモデルと特徴量選択結果を保存しました:\")\n",
    "print(f\"  - モデル: jq_model.joblib\")\n",
    "print(f\"  - 選択特徴量: jq_selected_features.joblib\") \n",
    "print(f\"  - 選択結果: feature_selection_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. モデルの評価\n",
    "def evaluate_model(model, X_test, y_test, selected_features):\n",
    "    \"\"\"\n",
    "    テストデータでモデルを評価\n",
    "    \"\"\"\n",
    "    dtest = xgb.DMatrix(X_test[selected_features])\n",
    "    y_pred = model.predict(dtest)\n",
    "    \n",
    "    # RMSEを計算（squared=Falseが使えない場合の代替方法）\n",
    "    try:\n",
    "        # 新しいバージョンのscikit-learn\n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    except TypeError:\n",
    "        # 古いバージョンのscikit-learn：MSEの平方根を取る\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\n=== テストデータでの評価結果 ===\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'predictions': y_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. テストデータで評価\n",
    "X_test = test_processed[feature_cols]\n",
    "y_test = test_processed['label']\n",
    "\n",
    "results = evaluate_model(final_model, X_test, y_test, selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 特徴量の重要度を表示\n",
    "def plot_feature_importance(model, selected_features, top_n=20):\n",
    "    \"\"\"\n",
    "    特徴量の重要度を表示\n",
    "    \"\"\"\n",
    "    importance = model.get_score(importance_type='gain')\n",
    "    \n",
    "    # DataFrameに変換\n",
    "    importance_df = pd.DataFrame(\n",
    "        [(f, importance.get(f'f{i}', 0)) for i, f in enumerate(selected_features)],\n",
    "        columns=['feature', 'importance']\n",
    "    )\n",
    "    importance_df = importance_df.sort_values('importance', ascending=False).head(top_n)\n",
    "    \n",
    "    print(f\"\\n=== Top {top_n} 重要な特徴量 ===\")\n",
    "    for idx, row in importance_df.iterrows():\n",
    "        print(f\"{row['feature']}: {row['importance']:.2f}\")\n",
    "    \n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 特徴量の重要度を表示\n",
    "importance_df = plot_feature_importance(final_model, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線の可視化（オプション）\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(evals_result['train']['rmse'], label='Train RMSE')\n",
    "plt.plot(evals_result['val']['rmse'], label='Validation RMSE')\n",
    "plt.xlabel('Boosting Rounds')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "x = y_test\n",
    "y = results['predictions']\n",
    "\n",
    "sns.scatterplot(x=x, y=y, s=5, color=\".15\")\n",
    "sns.histplot(x=x, y=y, bins=50, pthresh=.1, cmap=\"mako\")\n",
    "sns.kdeplot(x=x, y=y, levels=5, color=\"w\", linewidths=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量選択結果の詳細分析\n",
    "def analyze_feature_selection_results(original_features, selected_features, final_model):\n",
    "    \"\"\"\n",
    "    特徴量選択結果の詳細分析\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"特徴量選択結果の詳細分析\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 基本統計\n",
    "    print(f\"元の特徴量数: {len(original_features)}\")\n",
    "    print(f\"選択された特徴量数: {len(selected_features)}\")\n",
    "    print(f\"削減率: {(1 - len(selected_features)/len(original_features))*100:.1f}%\")\n",
    "    \n",
    "    # 特徴量タイプ別の分析\n",
    "    original_numeric = [f for f in original_features if f.startswith('ftn.')]\n",
    "    original_categorical = [f for f in original_features if f.startswith('ftc.')]\n",
    "    selected_numeric = [f for f in selected_features if f.startswith('ftn.')]\n",
    "    selected_categorical = [f for f in selected_features if f.startswith('ftc.')]\n",
    "    \n",
    "    print(f\"\\n特徴量タイプ別:\")\n",
    "    print(f\"  数値特徴量: {len(original_numeric)} → {len(selected_numeric)} ({len(selected_numeric)/len(original_numeric)*100:.1f}%)\")\n",
    "    print(f\"  カテゴリ特徴量: {len(original_categorical)} → {len(selected_categorical)} ({len(selected_categorical)/len(original_categorical)*100:.1f}%)\")\n",
    "    \n",
    "    # 重要度の上位特徴量\n",
    "    importance = final_model.get_score(importance_type='gain')\n",
    "    importance_df = pd.DataFrame(\n",
    "        [(f, importance.get(f'f{i}', 0)) for i, f in enumerate(selected_features)],\n",
    "        columns=['feature', 'importance']\n",
    "    )\n",
    "    importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\n最も重要な特徴量 Top 10:\")\n",
    "    for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n",
    "        feature_type = \"数値\" if row['feature'].startswith('ftn.') else \"カテゴリ\"\n",
    "        print(f\"  {i+1:2d}. {row['feature']} ({feature_type}): {row['importance']:.2f}\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# 分析実行\n",
    "importance_analysis = analyze_feature_selection_results(feature_cols, selected_features, final_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_performance_before_after_selection(X_train, y_train, X_val, y_val, \n",
    "                                              original_features, selected_features):\n",
    "    \"\"\"\n",
    "    特徴量選択前後の性能比較\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"特徴量選択前後の性能比較\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 元の特徴量での性能\n",
    "    print(\"全特徴量でのモデル学習中...\")\n",
    "    original_score, _ = train_xgboost_light(X_train, y_train, X_val, y_val, original_features, num_boost_round=200)\n",
    "    \n",
    "    # 選択特徴量での性能\n",
    "    print(\"選択特徴量でのモデル学習中...\")\n",
    "    selected_score, _ = train_xgboost_light(X_train, y_train, X_val, y_val, selected_features, num_boost_round=200)\n",
    "    \n",
    "    print(f\"\\n性能比較結果:\")\n",
    "    print(f\"  全特徴量 ({len(original_features)}個): RMSE = {original_score:.4f}\")\n",
    "    print(f\"  選択特徴量 ({len(selected_features)}個): RMSE = {selected_score:.4f}\")\n",
    "    print(f\"  性能変化: {((selected_score - original_score) / original_score * 100):+.2f}%\")\n",
    "    print(f\"  特徴量削減: {((len(original_features) - len(selected_features)) / len(original_features) * 100):.1f}%\")\n",
    "    \n",
    "    return original_score, selected_score\n",
    "\n",
    "# 性能比較実行\n",
    "original_performance, selected_performance = compare_performance_before_after_selection(\n",
    "    X_train, y_train, X_val, y_val, feature_cols, selected_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foregues",
   "language": "python",
   "name": "foregues"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
